{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rlfAJ5K2S_Q"
   },
   "source": [
    "# Bayes Classifier and Maximum Likelihood for a multivariate Gaussian density\n",
    "# Classificateur de Bayes et maximum de vraisemblance d'une loi gaussienne multivariée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eS0-skWZ2S_S"
   },
   "source": [
    "## Preface/Préface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-GU7DBd2S_T"
   },
   "source": [
    "Begin, if necessary, by recalling the course notes on the [Bayes classifier](https://studium.umontreal.ca/pluginfile.php/4027948/mod_resource/content/4/7_bayes_classifier-en.pdf) and the principle of [maximum likelihood](https://studium.umontreal.ca/pluginfile.php/4003963/mod_resource/content/4/5_gaussian_distribution_en.pdf).\n",
    "\n",
    "Remember that you need to **submit** a `.py` file in Gradescope (and **not** a jupyter notebook). You can either copy the code from the notebook's cells and paste it into a new file, or use Jupyter's tool to export to a `.py` file (File / Download as / Python). **The name of your submission HAS TO BE `solution.py`**. For the autograder to work properly, you also need to comment all `print` statements.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Commencez, si nécessaire, par faire une révision des notes de cours portant sur le [classificateur de Bayes](https://studium.umontreal.ca/pluginfile.php/4027948/mod_resource/content/4/7_bayes_classifier-en.pdf) et sur le principe du [maximum de vraisemblance](https://studium.umontreal.ca/pluginfile.php/4003963/mod_resource/content/4/5_gaussian_distribution_en.pdf).\n",
    "\n",
    "Souvenez-vous que vous devez **soumettre** un fichier `.py` sur Gradescope (et **non** un jupyter notebook). Vous pouvez soit copier le code des cellules du jupyter notebook et le coller dans un nouveau fichier `.py` ou utiliser la fonctionalité de jupyter notebook afin d'exporter le fichier en un fichier `.py` (File / Download as / Python). **Le nom du fichier que vous allez soumettre DOIT ÊTRE** `solution.py`. Pour que le correcteur automatique fonctionne correctement, vous devez commenter toutes les assertions `print` dans votre `.py`.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zl9aa2Gc2S_U"
   },
   "source": [
    "## High level description/Description détaillée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WmSTERQ2S_V"
   },
   "source": [
    "Today we are going to build a **multi-class Bayes classifier**. This means that instead of modeling $ p (\\mbox{class} \\ | \\ \\mbox{example}) $ (or $ p (y \\ | \\ x) $), we will instead use the Bayes equation\n",
    "\n",
    "$$ p (\\mbox{class} \\ | \\ \\mbox{example}) = \\frac{p (\\mbox{example} \\ | \\ \\mbox{class}) p (\\mbox {class})} {\\sum_{c'= 1}^{m} p_\\mbox{c'}(x) P_\\mbox{c'}} $$\n",
    "\n",
    "and model the different pieces. In fact, we just need to model the numerator since the denominator is a normalization constant. In addition, $ P_\\mbox{c '} = n_c / n $\n",
    "\n",
    "The term $ p (\\mbox{class}) $ represents the prior probability of a class, that is, our a priori belief - before we have seen a particular example - about the probability that an unknown example belongs to this class. We will represent this belief a priori for a class by the frequency of the latter in the training data: $ \\frac{n_c}{n} $ where $ n_c $ = number of examples of the class $ c $, and $ n $ = number of training examples.\n",
    "\n",
    "We will use **multivariate Gaussian densities** to model the different $ p (\\mbox{example} \\ | \\ \\mbox{class}) $. This means that **for each class** $c$, we will assume that the \"true\" distribution $ p (\\mbox{example} \\ | \\ \\mbox{class}) $ has the form of a multivariate Gaussian for which we will try to learn the parameters $ \\mu_c $ and $ \\Sigma_c $. In practice, we will limit ourselves today to a particular case of this distribution: the **isotropic Guassian**, i.e. the covariance matrix $ \\Sigma_c $ of each Gaussian is diagonal and that each element of this diagonal is the same ( we denote it `sigma_sq`, in order to represent the variance $ \\sigma^2 $). Thus we have a single parameter to control the expression of the covariance for each class. It's easier (for us and for the computer) to calculate, but it also means that our model is less powerful.\n",
    "\n",
    "Hence, we have a very simple parametric model for each class $c$. The parameters are the average $ \\mu_c $ (a vector of the same dimension as the dimension of the system input) and the variance $ \\sigma^2_c $ (a single scalar in our simple model, which will multiply the identity matrix). We are going to learn this model with the **maximum likelihood principle**. For each class, we will find the values of the parameters that maximize the log-likelihood of the training data from this class:\n",
    "\n",
    "$$ \\log \\prod_i^n p(X = x_i) $$\n",
    "\n",
    "For an isotropic Gaussian of dimension $d$, the maximum likelihood estimators of $\\mu$ and $\\sigma^2$ are given by: \n",
    "\n",
    "$$\\mu_{ML} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\n",
    "\n",
    "$$\\sigma_{ML}^2 = \\frac{1}{nd} \\sum_{i=1}^{n} (x_i-\\mu_{ML})^T(x_i-\\mu_{ML})$$\n",
    "\n",
    "Having found the parameters that maximize the likelihood for each class, we can calculate each $ p (\\mbox{example} \\ |  \\ \\mbox{class}) $. It is now sufficient to apply the Bayes rule in order to classify a new example. More precisely, we want to choose, for an example, the class that maximizes $ p(\\mbox{example} \\ | \\ \\mbox{class}) p(\\mbox{class}) $ or, equivalently, $ \\log (p (\\mbox{example } \\ | \\ \\mbox{class} ) p(\\mbox{class})) $. **At test time, our classifier needs to evaluate this quantity for each class, and use the obtained values to assign a class label**.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Aujourd'hui, nous allons construire un **classîficateur de Bayes multi-classes**. Ceci veut dire qu'à la place de modéliser  $ p (\\mbox{classe} \\ | \\ \\mbox{exemple}) $ (ou $ p (y \\ | \\ x) $), nous allons plutôt utiliser la formule de Bayes suivante: \n",
    "\n",
    "$$ p (\\mbox{classe} \\ | \\ \\mbox{exemple}) = \\frac{p (\\mbox{exemple} \\ | \\ \\mbox{classe}) p (\\mbox {classe})} {\\sum_{c'= 1}^{m} p_\\mbox{c'}(x) P_\\mbox{c'}} $$\n",
    "\n",
    "et allons modéliser différents termes. En fait, nous avons uniquement besoin de modéliser le numérateur puisque le dénominateur  est une constante de normalisation. De plus, $ P_\\mbox{c '} = n_c / n $.\n",
    "\n",
    "Le terme $ p (\\mbox{classe}) $ représente la probabilité à priori d'une classe, c'est-à-dire notre hypothèse à priori - avant avoir vu un certain exemple - de la probabilité qu'un exemple quelconque appartienne à cette classe. Nous allons représenter cette hypothèse par sa fréquence dans notre jeu d'entraînement: $ \\frac{n_c}{n} $, où $ n_c $ est le nombre d'exemples appartement à cette classe et $ n $ est le nombre d'exemples dans notre jeu d'entraînement.\n",
    "\n",
    "Nous allons utiliser des **gaussiennes multivariées** afin de modéliser les différentes probabilités $ p (\\mbox{exemple} \\ | \\ \\mbox{classe}) $. En d'autres mots, nous assumons que pour **chaque classe** $c$, la *vraie* distribution $ p (\\mbox{exemple} \\ | \\ \\mbox{classe}) $  possède la forme d'une loi gaussienne multivarié pour laquelle nous allons apprendre les paramètres $ \\mu_c $ et $ \\Sigma_c $. Nous allons nous limiter aujourd'hui à un cas particulier de cette distribution, soit la loi **gaussienne isotropique**. Plus précisément, la matrice de covariance $ \\Sigma_c $ de chacune de ces distributions est une matrice diagonale dont chacun des éléments sur la diagonale sont égaux (nous notons `sigma_sq` la variance $ \\sigma^2 $). Ainsi, nous n'avons qu'un seul paramètre à estimer afin de déterminer la matrice de covariance pour chaque classe. C'est certainement plus facile (pour nous et pour l'ordinateur) de calculer sa valeur, mais ceci veut également dire que notre modèle possède moins de capacité.\n",
    "\n",
    "Nous avons alors un modèle paramétrique assez simple pour chaque classe $c$. Les paramètres sont l'espérance $ \\mu_c $ (un vecteur de la même dimension que nos valeurs d'entrées) et la variance $ \\sigma^2_c $ (un scalaire multiplié par la matrice identité). Nous allons apprendre (estimer) ces paramètres à l'aide du maximum de vraisemblance. Pour chaque classe, nous trouverons les valeurs des paramètres qui maximisent la *log-vraisemblance* des points d'entraînement de cette classe: \n",
    "\n",
    "$$ \\log \\prod_i^n p(X = x_i) $$\n",
    "\n",
    "Pour une gaussienne isotropique de dimension $d$, les estimateurs du maximum de vraisemblance $\\mu$ et $\\sigma^2$ sont:\n",
    " \n",
    "\n",
    "$$\\mu_{ML} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\n",
    "\n",
    "$$\\sigma_{ML}^2 = \\frac{1}{nd} \\sum_{i=1}^{n} (x_i-\\mu_{ML})^T(x_i-\\mu_{ML})$$\n",
    "\n",
    "Après avoir trouvé les paramètres qui maximisent la vraisemblance pour chaque classe, nous pouvons ensuite calculer chaque probabilité $ p (\\mbox{exemple} \\ |  \\ \\mbox{classe}) $. Il est maintenant suffisant d'appliquer la règle de Bayes afin de classer un nouvel exemple. Plus précisément, nous voulons trouver, pour un exemple donné, la classe qui maximise $ p(\\mbox{exemple} \\ | \\ \\mbox{classe}) p(\\mbox{classe}) $ (ou de manière équivalente $ \\log (p (\\mbox{exemple } \\ | \\ \\mbox{classe} ) p(\\mbox{classe})) $. **Au moment test, notre classifieur doit évaluer cette quantité pour chaque classe et utiliser ces valeurs afin d'assigner une classe à l'exemple.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44PQHAbK2S_b"
   },
   "source": [
    "## Code to be completed/Code à compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0oRzkhCt2S_f"
   },
   "source": [
    "For the `GaussianMaxLikelihood` class:\n",
    " \n",
    "  - **Calculate** mu ($\\mu$) and sigma_sq ($ \\sigma^2 $), the mean and the variance in `GaussianMaxLikelihood.train`. \n",
    "  - **Calculate** the value of the Gaussian density function in `GaussianMaxLikelihood.loglikelihood`, store it in the variable `log_prob` and **return it**.\n",
    "  \n",
    "In `__init__`, we initialize `self.mu` and `self.sigma_sq`. But both variables will be updated when the `train` function is called.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Pour la classe `GaussianMaxLikelihood`:\n",
    "\n",
    "  - **Calculez** mu ($\\mu$) et sigma_sq ($ \\sigma^2 $), la moyenne et la variance dans `GaussianMaxLikelihood.train`.\n",
    "  - **Calculez** la densité de la loi gaussienne dans `GaussianMaxLikelihood.loglikelihood`, definissez la comme étant la variable `log_prob` et **retournez la**.\n",
    "  \n",
    "Dans `__init__`, nous initialisons `self.mu` et `self.sigma_sq`. Ces variables seront ensuite modifiées lorsque la fonction `train` sera appelée. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "094nMfAX2S_h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 257,
     "status": "error",
     "timestamp": 1569507136662,
     "user": {
      "displayName": "Devendra Singh Sachan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBQXF6-VhZrcoM-lzH0KF8vzhg1RHerVFyNAIJ7rA=s64",
      "userId": "10261664123906371054"
     },
     "user_tz": 240
    },
    "id": "_SXw79kz2S_k",
    "outputId": "f4923dd6-1d9e-4dba-ea8a-4d0bae0cb7cd"
   },
   "outputs": [],
   "source": [
    "class GaussianMaxLikelihood:\n",
    "    def __init__(self, n_dims):\n",
    "        self.n_dims = n_dims\n",
    "        self.mu = np.zeros(n_dims)\n",
    "        # We only save a scalar standard deviation because our model is the isotropic Gaussian\n",
    "        # We avons un scalaire comme écart-type car notre modèle est une loi gaussienne isotropique\n",
    "        self.sigma_sq = 1.0\n",
    "\n",
    "    # For a training set, the function should compute the ML estimator of the mean and the variance\n",
    "    # Pour un jeu d'entraînement, la fonction devrait calculer les estimateur ML de l'espérance et de la variance\n",
    "    def train(self, train_data):\n",
    "        # Here, you have to find the mean and variance of the train_data data and put it in self.mu and self.sigma_sq\n",
    "        # Ici, nous devons trouver la moyenne et la variance dans train_data et les définir dans self.mu and self.sigma_sq\n",
    "        \n",
    "        # ---> WRITE CODE HERE/ÉCRIVEZ VOTRE CODE ICI\n",
    "        self.mu = np.mean(train_data, axis=0)\n",
    "        self.sigma_sq = np.sum((train_data - self.mu) ** 2.0) / (self.n_dims * train_data.shape[0])\n",
    " \n",
    "        \n",
    "    # Returns a vector of size nb. of test ex. containing the log probabilities of each test example under the model.\n",
    "    # Retourne un vecteur de dimension égale au nombre d'ex. test qui contient les log probabilité de chaque \n",
    "    # exemple test\n",
    "    def loglikelihood(self, test_data):\n",
    "\n",
    "        # comment the following line once you have completed the calculation of log_prob\n",
    "        # mettez en commentaire cette ligne lorsque vous avez complétez le calcul de log_prob\n",
    "\n",
    "        # the following line calculates log(normalization constant)\n",
    "        # la ligne suivante calcule le log(normalization constant)\n",
    "        c = - ((self.n_dims / 2) * np.log(2 * np.pi)) - ( self.n_dims * np.log(np.sqrt(self.sigma_sq)) )\n",
    "        # It is necessary to calculate the value of the log-probability of each test example\n",
    "        # under the model determined by mu and sigma_sq. The vector of probabilities is / will be log_prob\n",
    "        # Il est nécessaire de calculer la log-probabilité de chaque exemple test sous le modèle déterminé\n",
    "        # par mu et sigma_sq. Le vecteur de probabilité est/sera log_prob\n",
    "        \n",
    "        # ---> WRITE CODE HERE/ÉCRIVEZ VOTRE CODE ICI\n",
    "        log_prob = c - np.sum((test_data - self.mu) ** 2.0, axis=1) / (2.0 * self.sigma_sq)\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJjOWRUh2S_q"
   },
   "source": [
    "For class `BayesClassifier`:\n",
    "\n",
    "  - **Complete** `bayes_classifier.loglikelihood`\n",
    "  \n",
    "`self.maximum_likelihood_models` is a list of size equal to the number of classes. Each element is a model that allows log likelihood evaluations using a `loglikelihood` method.\n",
    "\n",
    "`self.priors` is a list of scalars. The numbers represent the prior probabilities of the classes.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Pour la classe `BayesClassifier`:\n",
    "\n",
    "  - **Complétez** `bayes_classifier.loglikelihood`\n",
    "  \n",
    "`self.maximum_likelihood_models` est une liste de longueur égale au nombre de classes. Chaque élément est un modèle qui permet de calculer les log-vraisemblances en utilisant la méthode `loglikelihood`.\n",
    "\n",
    "`self.priors` est une liste de scalaires. Ils représentent les probabilités à priori des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEZKOQzi2S_r"
   },
   "outputs": [],
   "source": [
    "class BayesClassifier:\n",
    "    def __init__(self, maximum_likelihood_models, priors):\n",
    "        self.maximum_likelihood_models = maximum_likelihood_models\n",
    "        self.priors = priors\n",
    "        if len(self.maximum_likelihood_models) != len(self.priors):\n",
    "            print('The number of ML models must be equal to the number of priors!')\n",
    "        self.n_classes = len(self.maximum_likelihood_models)\n",
    "\n",
    "    # Returns a matrix of size number of test ex. times number of classes containing the log\n",
    "    # probabilities of each test example under each model, trained by ML.\n",
    "    # Retourne une matrice de dimension [nb d'ex. test, nb de classes] contenant les log\n",
    "    # probabilités de chaque ex. test sous le modèle entrainé par le MV.\n",
    "    def loglikelihood(self, test_data):\n",
    "\n",
    "        log_pred = np.zeros((test_data.shape[0], self.n_classes))\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            # Here, we will have to use maximum_likelihood_models[i] and priors to fill in\n",
    "            # each column of log_pred (it's more efficient to do a entire column at a time)\n",
    "            # Ici, nous devrons utiliser maximum_likelihood_models[i] et priors pour remplir\n",
    "            # chaque colonne de log_pred (c'est plus efficace de remplir une colonne à la fois) \n",
    "            \n",
    "            # ---> WRITE CODE HERE/ÉCRIVEZ VOTRE CODE ICI\n",
    "            log_pred[:, i] = self.maximum_likelihood_models[i].loglikelihood(test_data) + np.log(self.priors[i]) \n",
    "\n",
    "        return log_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTeHVkjc2S_v"
   },
   "source": [
    "In this part, we will train a BayesClassifier on the iris dataset, according to the what we described in the High level description section. We will use the classes BayesClassifier and GaussianMaxLikelihood.\n",
    "\n",
    "Contrary to what we did in the previous labs, **we will not use a train/test set**. We will simply use the whole dataset to train the classifier, and we are going to ask the trained classifier to **make predictions on the training set itself**. \n",
    "\n",
    "There are extra questions at the end of this notebook where you can play around and use a proper train/test split. But this will not be tested by gradescope.\n",
    "\n",
    "**Complete** the function `get_accuracy(test_inputs, test_labels)`, that takes as input a test set, and returns the trained classifier's (`classifier`) accuracy on this test set. Your function should return a number between 0 and 1.\n",
    "\n",
    "To make sure everything works fine, you can test your code by calling your function on the train dataset (This part of the code after the function is provided for you). But remember that your function should work with any inputs, and not only (iris_train, iris_labels).\n",
    "\n",
    "<hr>\n",
    "\n",
    "Dans cette partie, nous allons entraîner BayesClassifier sur le dataset iris, tel qu'expliqué dans la description détaillée. Nous allons utiliser les classes BayesClassifier et GaussianMaxLikelihood.\n",
    "\n",
    "Contrairement à ce que nous avons fait dans le lab précédent, **nous n'allons pas utiliser un dataset train/test**. Nous allons simplement utiliser le dataset complet pour entraîner le modèle et ensuite **faire des prédictions sur le training set lui-même.**\n",
    "\n",
    "Des questions bonus se trouvent à la fin de ce notebook pour que vous puissiez explorer avec un ensemble d'entraînement/test. Ces questions ne seront cependant pas testées sur gradescope.\n",
    "\n",
    "**Complétez** la fonction `get_accuracy(test_inputs, test_labels)`, qui prend en paramètre un ensemble test, et retourne l'exactitude du classifieur (`classifier`) entraîné. Votre fonction devrait retourner un nombre entre 0 et 1.\n",
    "\n",
    "Pour vous assurer que tout fonctionne, vous pouvez tester votre code par appeller votre fonction sur l'ensemble d'entraînement (cette partie du code, après la fonction, vous est donnée). Souvenez-vous cependant que votre fonction devrait fonctionner avec toutes données d'entrée, et pas seulement (iris_train, iris_labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gGAH3AHd2S_x"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "iris.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-49b375e3a275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0miris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://www.iro.umontreal.ca/~dift3395/files/iris.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0miris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iris.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0miris_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0miris_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: iris.txt not found."
     ]
    }
   ],
   "source": [
    "# We load the dataset and split it into input data, and labels\n",
    "# On télécharge le dataset et le séparons en input et en étiquettes\n",
    "import sys\n",
    "import numpy as np\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    iris = np.loadtxt('http://www.iro.umontreal.ca/~dift3395/files/iris.txt')\n",
    "else:\n",
    "    iris = np.loadtxt('http://www.iro.umontreal.ca/~dift3395/files/iris.txt')\n",
    "iris_train = iris[:, :-1]\n",
    "iris_labels = iris[:, -1]\n",
    "\n",
    "# We split the input data into three sub-datasets, corresponding to the classes 1, 2, and 3. \n",
    "# It is necessary to make such a split, in order to train a GaussianMaxLikelihood model per class\n",
    "# Note that the split (0:50, 50:100, 100:150) is not arbitrary. It corresponds to the three classes in the dataset\n",
    "# On sépare le input en trois sous-ensembles, correspondant aux classes 1, 2 et 3. Il est nécessaire\n",
    "# de faire cette partition afin d'entraîner GaussianMaxLikelihood pour chaque classe. Notez que la partition\n",
    "# (0:50, 50:100 et 100:150) n'est pas arbitraire: elle correspond aux trois classes dans le dataset.\n",
    "iris_train1 = iris_train[0:50, :]\n",
    "iris_train2 = iris_train[50:100, :]\n",
    "iris_train3 = iris_train[100:150, :]\n",
    "\n",
    "# We create a model per class (using maximum likelihood), and train each of them using the corresponding data\n",
    "# Nous créons un modèle par classe (en utilisant le maximum de vraisemblance) et les entraînons\n",
    "model_class1 = GaussianMaxLikelihood(4)\n",
    "model_class2 = GaussianMaxLikelihood(4)\n",
    "model_class3 = GaussianMaxLikelihood(4)\n",
    "model_class1.train(iris_train1)\n",
    "model_class2.train(iris_train2)\n",
    "model_class3.train(iris_train3)\n",
    "\n",
    "# We create a list of all our models, and the list of prior values\n",
    "# Here the priors are calculated exactly because we know the number of representatives per class.\n",
    "# Nous créons une liste de nos modèles et une liste de nos priors. Ici, nos priors sont\n",
    "# déjà calculés car nous savons les proportions par classe\n",
    "model_ml = [model_class1, model_class2, model_class3]\n",
    "priors = [1./3, 1./3, 1./3]\n",
    "\n",
    "# We create our classifier with our list of Gaussian models and our priors\n",
    "# Nous créons notre classifieur avec notre liste de modèles gaussiens et nos prob à priori\n",
    "classifier = BayesClassifier(model_ml, priors)\n",
    "\n",
    "# Returns a number between 0 and 1 representing the accuracy of the model on the test_inputs\n",
    "# Retourne un nombre entre 0 et 1 représentant l'exactitude du modèle sur les test_inputs\n",
    "def get_accuracy(test_inputs, test_labels):\n",
    "    # We can calculate the log-probabilities according to our model\n",
    "    # Nous pouvons calculez les log-probabilités selon notre modèle\n",
    "    \n",
    "    # ---> WRITE CODE HERE/ÉCRIVEZ VOTRE CODE ICI\n",
    "    log_prob = classifier.loglikelihood(test_inputs)\n",
    "    # It now remains to calculate the predicted labels\n",
    "    # Il reste à calculer les classes prédites\n",
    "    \n",
    "    # ---> WRITE CODE HERE/ECRIVEZ VOTRE CODE ICI\n",
    "    classes_pred = log_prob.argmax(1) + 1\n",
    "    # Return the accuracy by comparing your predicted labels to the actual labels\n",
    "    # Retournez l'exactitude en comparant les classes prédites aux vraies étiquettes\n",
    "\n",
    "    # ---> WRITE CODE HERE/ECRIVEZ VOTRE CODE ICI\n",
    "    return np.mean(classes_pred == test_labels)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    print(\"The training accuracy is : {:.1f} % \".format(100 * get_accuracy(iris_train, iris_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vzy_ryyk2S_1"
   },
   "source": [
    "## Once you're done/Une fois terminé (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rs3Ux-Ar2S_2"
   },
   "source": [
    "- Change your code so that `GaussianMaxLikelihood` calculates 1) a diagonal covariance matrix (where we estimate the variance for each component / trait of the input) 2) a full covariance matrix. You can for example have a parameter `cov_type` that can be \"isotropic\", \"diagonal\" or \"full\". The `numpy.cov` and` numpy.var` commands will probably be useful.\n",
    "\n",
    "- Instead of using the whole iris dataset to train a `BayesClassifier`, peform a random split of the data (2/3, 1/3) to obtain a train set and a test set, and use the train set to train your classifier. Compare the obtained accuracies on the train set and the test set. Remember to adjust the code that defines `iris_train1`, `iris_train2`, `iris_train3`, and `priors` accordingly. Compare the train and test accuracies of 3 models: one that uses \"isotropic\" gaussian models, one that uses \"diagonal\" gaussian models, and one that uses \"full\" gaussian models.\n",
    "\n",
    "- Instead of using the 4 features to train the classifier, use only 2 and try to make nice visualizations as we did in Lab 2. You can of course use the utility functions from there to make your plots.\n",
    "\n",
    "<hr>\n",
    "\n",
    "- Changez votre code afin que `GaussianMaxLikelihood` calcule 1) une matrice de variance-covariance diagonale (où on estime la variance de chaque élément/trait des données d'entrée) 2) une matrice de variance-covariance complète. Vous pouvez par exemple avoir un paramètre `cov_type` qui peut être `isotropic`, `diagonal` ou `full`. Les commandes `numpy.cov` et `numpy.var` peuvent être utiles.\n",
    "\n",
    "- À la place d'utiliser le dataset complet iris pour entraîner `BayesClassifier`, performez une séparation aléatoire de (2/3, 1/3) afin d'obtenir un ensemble d'entraînement et un ensemble test. Comparez les exactitudes du classifieur obtenues à partir des ensembles train et test. Prenez soin d'ajuster le code qui définit `iris_train1`, `iris_train2`, `iris_train3` et `priors` en conséquence. Comparez l'exactitude des ensembles train et test des 3 modèles: gaussienne isotropique, gaussienne diagonale et gaussienne *complète*.\n",
    "\n",
    "- Plutôt que d'utiliser les 4 traits afin d'entraîner votre classifieur, utilisez seulement que deux caractéristiques et effectuer des analyses de visualisation telles que faites dans le lab 2. Vous pouvez biensûr faire usages des fonctions du lab 2 pour faire vos graphiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# ANSWER TO THE BONUS QUESTION\n",
    "# (MAKE SURE YOU DELETE THIS PART BEFORE YOU MAKE A SUBMISSION ON GRADESCOPE)\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "class GaussianMaxLikelihood:\n",
    "    def __init__(self,n_dims,cov_type=\"isotropic\"):\n",
    "        self.n_dims = n_dims\n",
    "        self.mu = np.zeros((1,n_dims))\n",
    "        self.cov_type = cov_type\n",
    "\n",
    "        if cov_type==\"isotropic\":\n",
    "            self.sigma_sq = 1.0\n",
    "        elif cov_type==\"diagonal\":\n",
    "            self.sigma_sq = np.ones(n_dims)\n",
    "        if cov_type==\"full\":\n",
    "            self.cov = np.ones((n_dims,n_dims))\n",
    "    # For a training set, the function should compute the ML estimator of the mean and the covariance matrix\n",
    "    def train(self, train_data):\n",
    "        self.mu = np.mean(train_data,axis=0)\n",
    "        if self.cov_type == \"isotropic\":\n",
    "            self.sigma_sq = np.sum((train_data - self.mu)**2.0) / (self.n_dims * train_data.shape[0])\n",
    "        elif self.cov_type == \"diagonale\":\n",
    "            self.sigma_sq =  np.sum((train_data - self.mu) ** 2.0, axis = 0) / train_data.shape[0]\n",
    "        if self.cov_type == \"full\":\n",
    "            self.cov = np.cov(np.transpose(train_data))\n",
    "\n",
    "    # Returns a vector of size nb. of test ex. containing the log\n",
    "    # probabilities of each test example under the model.\n",
    "    def loglikelihood(self, test_data):\n",
    "\n",
    "        if self.cov_type == \"isotropic\":\n",
    "            # the following line calculates log(normalization constant)\n",
    "            c = -self.n_dims * np.log(2*np.pi)/2 - self.n_dims*np.log(np.sqrt(self.sigma_sq))\n",
    "            # it is necessary to calculate the value of the log-probability of each test example\n",
    "            #under the model determined by mu and sigma_sq\n",
    "            #the vector of probabilities is / will be log_prob\n",
    "            log_prob = c - np.sum((test_data -  self.mu)**2.0,axis=1) / (2.0 * self.sigma_sq)\n",
    "        elif self.cov_type == \"diagonal\":\n",
    "            # we take the product of the vector representing the diagonal (np.prod(self.sigma)\n",
    "            c = -self.n_dims * np.log(2*np.pi)/2.0 - np.log(np.prod(self.sigma_sq))/2.0\n",
    "            # we do a summation along axis 1 after having divided by sigma because the latte is also\n",
    "            # of dimension d\n",
    "            log_prob = c - np.sum((test_data -  self.mu)**2.0/ (2.0 * self.sigma_sq),axis=1)\n",
    "        elif self.cov_type == \"full\":\n",
    "            c = -self.n_dims * np.log(2.0*np.pi)/2.0\n",
    "            det = np.linalg.det(self.cov)\n",
    "            c += np.log(det)/2.0\n",
    "\n",
    "            dmu = test_data-self.mu\n",
    "            inv = np.linalg.inv(self.cov)\n",
    "            dxs = np.dot(dmu,inv)\n",
    "            dxsx = np.sum(dxs*dmu,axis=1)\n",
    "            log_prob = c - dxsx\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "class BayesClassifier:\n",
    "\n",
    "    def __init__(self,models_ml, priors):\n",
    "        self.models_ml = models_ml\n",
    "        self.priors = priors\n",
    "        if len(self.models_ml) != len(self.priors):\n",
    "            print('The number of ML models must be equal to the number of priors!')\n",
    "        self.n_classes = len(self.models_ml)\n",
    "\n",
    "    # Returns a matrix of size nb. of test ex. times number of classes containing the log\n",
    "    # probabilities of each test example under each model, trained by ML.\n",
    "    def loglikelihood(self, test_data, eval_by_group=False):\n",
    "\n",
    "        log_pred = np.empty((test_data.shape[0],self.n_classes))\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            # Here we will have to use modeles_mv [i] and priors to fill in\n",
    "            # each column of log_pred (it's more efficient to do a entire column at a time)\n",
    "\n",
    "            log_pred[:,i] = self.models_ml[i].loglikelihood(test_data) +  np.log(self.priors[i])\n",
    "\n",
    "        return log_pred\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# PERFORM A (2/3, 1/3) RANDOM SPLIT OF DATA\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "# Here we provide an example where we do not divide the data into train and test set.\n",
    "iris=np.loadtxt('iris.txt')\n",
    "np.random.seed(123)\n",
    "\n",
    "indices1 = np.arange(0,50)\n",
    "indices2 = np.arange(50,100)\n",
    "indices3 = np.arange(100,150)\n",
    "\n",
    "np.random.shuffle(indices1)\n",
    "np.random.shuffle(indices2)\n",
    "np.random.shuffle(indices3)\n",
    "\n",
    "iris_train1 = iris[indices1[:33]]\n",
    "iris_test1 = iris[indices1[33:]]\n",
    "iris_train2 = iris[indices2[:33]]\n",
    "iris_test2 = iris[indices2[33:]]\n",
    "iris_train3 = iris[indices3[:33]]\n",
    "iris_test3 = iris[indices3[33:]]\n",
    "\n",
    "iris_train = np.concatenate([iris_train1, iris_train2, iris_train3])\n",
    "iris_test = np.concatenate([iris_test1, iris_test2, iris_test3])\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# EXPERIMENTS WITH 4 FEATURES\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "train_cols = [0,1,2,3]\n",
    "\n",
    "for cov_type in ['isotropic','diagonal','full']:\n",
    "    # We create a model per class (using maximum likelihood)\n",
    "    model_class1=GaussianMaxLikelihood(len(train_cols),cov_type=cov_type)\n",
    "    model_class2=GaussianMaxLikelihood(len(train_cols),cov_type=cov_type)\n",
    "    model_class3=GaussianMaxLikelihood(len(train_cols),cov_type=cov_type)\n",
    "    model_class1.train(iris_train1[:,train_cols])\n",
    "    model_class2.train(iris_train2[:,train_cols])\n",
    "    model_class3.train(iris_train3[:,train_cols])\n",
    "\n",
    "    # We create a list of all our models\n",
    "    # We do the same thing for the priors\n",
    "    # Here the priors are calculated exactly because we know the number of representatives per class.\n",
    "    # Once you have created a train / test set, they need to be calculated exactly\n",
    "    model_ml=[model_class1,model_class2,model_class3]\n",
    "    priors=[0.3333,0.3333,0.3333]\n",
    "\n",
    "    # We create our classifier with our list of Gaussian models and our priors\n",
    "    classifier=BayesClassifier(model_ml,priors)\n",
    "\n",
    "    # we can now calculate the log-probabilities according to our models\n",
    "    log_prob_train=classifier.loglikelihood(iris_train[:, train_cols])\n",
    "    log_prob_test=classifier.loglikelihood(iris_test[:, train_cols])\n",
    "\n",
    "    # it now remains to calculate the maximum per class for the classification\n",
    "    classesPred_train = log_prob_train.argmax(1)+1\n",
    "    classesPred_test = log_prob_test.argmax(1)+1\n",
    "\n",
    "    print(cov_type)\n",
    "    print(\"Error rate (training) %.2f%%\" % ((1-(classesPred_train==iris_train[:,-1]).mean())*100.0))\n",
    "    print(\"Error rate (test) %.2f%%\" % ((1-(classesPred_test==iris_test[:,-1]).mean())*100.0))\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# VISUALIZATION FUNCTIONS\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "## http://code.activestate.com/recipes/302478/\n",
    "def combine(*seqin):\n",
    "    '''returns a list of all combinations of argument sequences.\n",
    "for example: combine((1,2),(3,4)) returns\n",
    "[[1, 3], [1, 4], [2, 3], [2, 4]]'''\n",
    "    def rloop(seqin,listout,comb):\n",
    "        '''recursive looping function'''\n",
    "        if seqin:                       # any more sequences to process?\n",
    "            for item in seqin[0]:\n",
    "                newcomb=comb+[item]     # add next item to current comb\n",
    "                # call rloop w/ rem seqs, newcomb\n",
    "                rloop(seqin[1:],listout,newcomb)\n",
    "        else:                           # processing last sequence\n",
    "            listout.append(comb)        # comb finished, add to list\n",
    "    listout=[]                      # listout initialization\n",
    "    rloop(seqin,listout,[])         # start recursive process\n",
    "    return listout\n",
    "\n",
    "def gridplot(classifier,train,test,n_points=50):\n",
    "\n",
    "    train_test = np.vstack((train,test))\n",
    "    (min_x1,max_x1) = (min(train_test[:,0]),max(train_test[:,0]))\n",
    "    (min_x2,max_x2) = (min(train_test[:,1]),max(train_test[:,1]))\n",
    "\n",
    "    xgrid = np.linspace(min_x1,max_x1,num=n_points)\n",
    "    ygrid = np.linspace(min_x2,max_x2,num=n_points)\n",
    "\n",
    "    # calculates the Cartesian product between two lists\n",
    "    # and puts the results in an array\n",
    "    thegrid = np.array(combine(xgrid,ygrid))\n",
    "\n",
    "    the_accounts = classifier.loglikelihood(thegrid)\n",
    "    classesPred = np.argmax(the_accounts,axis=1)+1\n",
    "\n",
    "    # The grid\n",
    "    # So that the grid is prettier\n",
    "    #props = dict( alpha=0.3, edgecolors='none' )\n",
    "    plt.scatter(thegrid[:,0],thegrid[:,1],c = classesPred, s=50)\n",
    "    # The training points\n",
    "    plt.scatter(train[:,0], train[:,1], c = 'r', marker = 'v', s=6)\n",
    "    # The test points\n",
    "    plt.scatter(test[:,0], test[:,1], c = 'm', marker = 's', s=6)\n",
    "\n",
    "    ## A little hack, because the functionality is missing at pylab ...\n",
    "    h1 = plt.plot([min_x1], [min_x2], marker='o', c = 'w',ms=6, label='grille')\n",
    "    h2 = plt.plot([min_x1], [min_x2], marker='v', c = 'r',ms=6, label='train')\n",
    "    h3 = plt.plot([min_x1], [min_x2], marker='s', c = 'm',ms=6, label='test')\n",
    "    #handles = [h1,h2,h3]\n",
    "    ## end of the hack\n",
    "    #labels = ['grille','train','test']\n",
    "    #plt.legend(handles,labels)\n",
    "    plt.legend(borderaxespad=0.)\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# EXPERIMENTS WITH 2 features\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "train_cols = [2,3]\n",
    "\n",
    "for cov_type in ['isotropic','diagonal','full']:\n",
    "    # We create a model per class (using maximum likelihood)\n",
    "    model_class1=GaussianMaxLikelihood(len(train_cols),cov_type=cov_type)\n",
    "    model_class2=GaussianMaxLikelihood(len(train_cols),cov_type=cov_type)\n",
    "    model_class3=GaussianMaxLikelihood(len(train_cols),cov_type=cov_type)\n",
    "    model_class1.train(iris_train1[:,train_cols])\n",
    "    model_class2.train(iris_train2[:,train_cols])\n",
    "    model_class3.train(iris_train3[:,train_cols])\n",
    "\n",
    "    # We create a list of all our models\n",
    "    # We do the same thing for the priors\n",
    "    # Here the priors are calculated exactly because we know the number of representatives per class.\n",
    "    # Once you have created a train / test set, they need to be calculated exactly\n",
    "    model_ml=[model_class1,model_class2,model_class3]\n",
    "    priors=[0.3333,0.3333,0.3333]\n",
    "\n",
    "    # We create our classifier with our list of Gaussian models and our priors\n",
    "    classifier=BayesClassifier(model_ml,priors)\n",
    "\n",
    "    # we can now calculate the log-probabilities according to our models\n",
    "    log_prob_train=classifier.loglikelihood(iris_train[:, train_cols])\n",
    "    log_prob_test=classifier.loglikelihood(iris_test[:, train_cols])\n",
    "\n",
    "    # it now remains to calculate the maximum per class for the classification\n",
    "    classesPred_train = log_prob_train.argmax(1)+1\n",
    "    classesPred_test = log_prob_test.argmax(1)+1\n",
    "\n",
    "    print(cov_type)\n",
    "    print(\"Error rate (training) %.2f%%\" % ((1-(classesPred_train==iris_train[:,-1]).mean())*100.0))\n",
    "    print(\"Error rate (test) %.2f%%\" % ((1-(classesPred_test==iris_test[:,-1]).mean())*100.0))\n",
    "\n",
    "    gridplot(classifier,\n",
    "             iris_train[:, train_cols + [-1]],\n",
    "             iris_test[:, train_cols + [-1]],\n",
    "             n_points=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3 - Bayes Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
